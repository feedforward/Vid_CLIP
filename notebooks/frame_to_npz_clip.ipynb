{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_FULL_PIPELINE_SHRITI (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XEZIE36gc5"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pja0alchWpiA",
        "outputId": "a4e2da16-7b17-4770-c708-faa11e4e7307"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnqPRvP5CRrH"
      },
      "source": [
        "# Install CLIP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBVr18E5tse8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704f1c36-edfd-4d85-c28f-aae0dd5c936e"
      },
      "source": [
        "# ! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "# # # The following command installs the `clip` module from its source:\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-lsgacphj\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-lsgacphj\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.41.1)\n",
            "Collecting torch~=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[?25hCollecting torchvision~=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-cp37-none-any.whl size=1368708 sha256=30c59a682beeebfabab3e04c0e85f2c2a4d1b9f43dc7880f33ac0cae632f490e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-81n12o2s/wheels/79/51/d7/69f91d37121befe21d9c52332e04f592e17d1cabc7319b3e09\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=021f6f34f476dd35ace70c4b09ae38bbe33ad9de6d840e7693357ef15c154279\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "Successfully built clip ftfy\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ftfy, torch, torchvision, clip\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed clip-1.0 ftfy-6.0.1 torch-1.7.1 torchvision-0.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9dgq7q6AQ9n"
      },
      "source": [
        "# Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn9cAF4CA6tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7664f87-79d7-4107-833a-4544ab44be34"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"cuda\", cuda)\n",
        "num_workers = 8 if cuda else 0\n",
        "print(num_workers)\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda True\n",
            "8\n",
            "Torch version: 1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy7P1dQokAz4"
      },
      "source": [
        "# Load CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFS29hnhlY4",
        "outputId": "fb9c957e-23bc-4c43-9eb3-808be4b84b38"
      },
      "source": [
        "print(\"Avaliable Models: \", clip.available_models())\n",
        "model, preprocess = clip.load(\"RN50\") # clip.load(\"ViT-B/32\") #\n",
        "\n",
        "input_resolution = model.input_resolution #.item()\n",
        "context_length = model.context_length #.item()\n",
        "vocab_size = model.vocab_size #.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avaliable Models:  ['RN50', 'RN101', 'RN50x4', 'ViT-B/32']\n",
            "Model parameters: 102,007,137\n",
            "Input resolution: tensor(224, device='cuda:0')\n",
            "Context length: tensor(77, device='cuda:0')\n",
            "Vocab size: tensor(49408, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqcK5FsoRYbN"
      },
      "source": [
        "# Selected classes and mapping for kinetics dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfzZSBWEQgJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0546317-c86f-4679-e1ae-c916a8d95e3b"
      },
      "source": [
        "labels = ['making tea',\n",
        " 'shaking head',\n",
        " 'skiing slalom',\n",
        " 'bobsledding',\n",
        " 'high kick',\n",
        " 'scrambling eggs',\n",
        " 'bee keeping',\n",
        " 'swinging on something',\n",
        " 'washing hands',\n",
        " 'laying bricks',\n",
        " 'push up',\n",
        " 'doing nails',\n",
        " 'massaging legs',\n",
        " 'using computer',\n",
        " 'clapping',\n",
        " 'drinking beer',\n",
        " 'eating chips',\n",
        " 'riding mule',\n",
        " 'petting animal (not cat)',\n",
        " 'frying vegetables',\n",
        " 'skiing (not slalom or crosscountry)',\n",
        " 'snowkiting',\n",
        " 'massaging person’s head',\n",
        " 'cutting nails',\n",
        " 'picking fruit']\n",
        "\n",
        "map_id = {}\n",
        "i=0\n",
        "for label in labels:\n",
        "  map_id[label]=i\n",
        "  i+=1\n",
        "\n",
        "map_id"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bee keeping': 6,\n",
              " 'bobsledding': 3,\n",
              " 'clapping': 14,\n",
              " 'cutting nails': 23,\n",
              " 'doing nails': 11,\n",
              " 'drinking beer': 15,\n",
              " 'eating chips': 16,\n",
              " 'frying vegetables': 19,\n",
              " 'high kick': 4,\n",
              " 'laying bricks': 9,\n",
              " 'making tea': 0,\n",
              " 'massaging legs': 12,\n",
              " 'massaging person’s head': 22,\n",
              " 'petting animal (not cat)': 18,\n",
              " 'picking fruit': 24,\n",
              " 'push up': 10,\n",
              " 'riding mule': 17,\n",
              " 'scrambling eggs': 5,\n",
              " 'shaking head': 1,\n",
              " 'skiing (not slalom or crosscountry)': 20,\n",
              " 'skiing slalom': 2,\n",
              " 'snowkiting': 21,\n",
              " 'swinging on something': 7,\n",
              " 'using computer': 13,\n",
              " 'washing hands': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax_TFW9MRlRK",
        "outputId": "fcc9c888-3311-4229-d31a-72cebcbe22f5"
      },
      "source": [
        "cd /content/drive/MyDrive/kinetics-downloader/TEST"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1hoNs0LbTnHoZWuEEYEXKCJH83Xbu67ZD/kinetics-downloader/TEST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "vc4Ff4pKSo0O",
        "outputId": "09ca97c2-788d-4d1c-df31-c760db447562"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "ROOT = \"/content/drive/MyDrive/kinetics-downloader/NEW_PROCESSED\"\n",
        "DEST = \"/content/drive/MyDrive/kinetics-downloader/TEST\"\n",
        "cnt = 1\n",
        "for filename in os.listdir(ROOT):\n",
        "  if filename in labels:\n",
        "    class_file = os.path.join(ROOT,filename)\n",
        "    for random_tag in os.listdir(class_file):\n",
        "      print(\"what is random\",random_tag)\n",
        "      video_name = os.path.join(class_file,random_tag)\n",
        "      filename = filename.replace(\"_\",\" \")\n",
        "      count = 0\n",
        "      N = len(os.listdir(video_name))\n",
        "      n = N//100\n",
        "      selected_frames = np.arange(0,N,n).tolist()[0:100]\n",
        "      for frame in os.listdir(video_name):\n",
        "        if count in selected_frames:\n",
        "          tmp = str(count)\n",
        "          image = Image.open(os.path.join(video_name,frame))\n",
        "          image = preprocess(image)\n",
        "          image = torch.unsqueeze(image, 0)\n",
        "          image = image.cuda()\n",
        "          image_features = model.encode_image(image)\n",
        "          image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features.detach().cpu().numpy()\n",
        "          if count==0:\n",
        "            a = image_features\n",
        "          else:\n",
        "            a = np.vstack((a,image_features))\n",
        "        count += 1\n",
        "      cnt+=1    \n",
        "\n",
        "      label = map_id[filename]*np.ones(a.shape[0])\n",
        "      file_save = os.path.join(DEST,str(cnt))\n",
        "      np.savez(file_save, data=a,label=label)\n",
        "    cnt+=1    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what is random 1\n",
            "what is random 2\n",
            "what is random 3\n",
            "what is random 4\n",
            "what is random 5\n",
            "what is random 6\n",
            "what is random 7\n",
            "what is random 8\n",
            "what is random 9\n",
            "what is random 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-77e5ed8db342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mselected_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmyb1A98DZ6O"
      },
      "source": [
        "# DEVELOP LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tobEOIE4DcPA"
      },
      "source": [
        "# LSTM Model\n",
        "NUM_CLASSES = 18\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_feature_size, embed_size, out_phoeneme, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(input_feature_size, embed_size , kernel_size=3,padding=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.lstm = nn.LSTM(1024, hidden_size, num_layers=5,bidirectional=True)\n",
        "        self.output1 = nn.Linear(hidden_size * 2, 256)\n",
        "        self.output3 = nn.Linear(256,NUM_CLASSES)\n",
        "    \n",
        "    def forward(self, X, lengths):\n",
        "        X_ = torch.transpose(X,0,1)\n",
        "        X_ = torch.transpose(X_,1,2)\n",
        "        X = self.layer1(X_)\n",
        "        X = torch.transpose(X,0,2)\n",
        "        X = torch.transpose(X,1,2)\n",
        "        # Store the length \n",
        "        packed_X = pack_padded_sequence(X, lengths.cpu(), enforce_sorted=False)\n",
        "        packed_out = self.lstm(packed_X)[0]\n",
        "        out, out_lens = pad_packed_sequence(packed_out)\n",
        "        out = self.output1(out)\n",
        "        out = self.output3(out).log_softmax(2) \n",
        "        return out, out_lens\n",
        "    \n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
        "    torch.nn.init.xavier_normal_(m.weight.data)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp9kO_-M9XT0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}